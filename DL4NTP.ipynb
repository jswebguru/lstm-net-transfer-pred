{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL4NTP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.4 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.4"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cCDCp4LhGom"
      },
      "source": [
        "#Import tensorflow and numpy\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrwM3cXKhYmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bad1d2e-951e-4613-b6ab-001787567e9a"
      },
      "source": [
        "#Import keras models\n",
        "from random import random\n",
        "from numpy import array\n",
        "from numpy import cumsum\n",
        "\n",
        "from keras.layers import Bidirectional\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI4XzmhYM7mh"
      },
      "source": [
        "#Import scikit modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmltwXVFCzKO"
      },
      "source": [
        "#Import any other libraries needed\n",
        "import math\n",
        "import os\n",
        "from datetime import datetime, date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O545X4ogU63c"
      },
      "source": [
        "# Read in Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apW7Ec7Q6cZU"
      },
      "source": [
        "1. Mount Google Drive file system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CHuCizHipju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4bc5d0-243f-48e3-c098-8ced3f965ee1"
      },
      "source": [
        "from google.colab import drive # Hosting the file in Google Drive, need to mount the drive so it is accessible\n",
        "# Currently Google forces an authorisation code, local runtime would rectify this\n",
        "drive.mount('/content/gdrive', force_remount=True) #force_remount forces Google t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWIZCG-u6Z12"
      },
      "source": [
        "2. Read in SANReN sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMMf07H7mA3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37963be7-c76c-4f9e-bc89-f12777bd2ba7"
      },
      "source": [
        "with open('SANREN_large.txt') as f:\n",
        "  SANReN = f.readlines()\n",
        "  \n",
        "#Iterate through first 5 rows to ensure data has been read correctly. \n",
        "for i in range(5):\n",
        "  print(SANReN[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QMDbVDehWb7"
      },
      "source": [
        "# ***Data Preprocessing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U64MBm826WiF"
      },
      "source": [
        "3. Clean dataframe headers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn5UvaMAtfxu"
      },
      "source": [
        "headings_line = SANReN[0].split()\n",
        "#Merge 'Src', 'IP', and 'Addr:Port' \n",
        "headings_line[4:7] = [''.join(headings_line[4:7])]\n",
        "#Merge 'Dst', 'IP', and 'Addr:Port' \n",
        "headings_line[5:8] = [''.join(headings_line[5:8])]\n",
        "#Remove 'Flags', 'Tos', and 'Flows'.\n",
        "headings_line = headings_line[0:6] + headings_line[8:13]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwj9a3jl6K70"
      },
      "source": [
        "4. Clean time-series data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6uYg9vVuEcc"
      },
      "source": [
        "framedata = []\n",
        "for i in range(1, 501):\n",
        "  data_line = SANReN[i].split()\n",
        "\n",
        "  if (data_line[11] == \"M\" and (data_line[14] == 'M' or data_line[14] == 'G')): #Bytes and BPS in megabytes\n",
        "    #print(\"1 and 2\") \n",
        "    \n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:11] + data_line[12:14] + data_line[15:16]\n",
        "    data_line[7] = float(data_line[7])*1000000 #Change M bytes into byte measurement. \n",
        "    if (data_line[9] == 'G'):\n",
        "      data_line[9] = float(data_line[9])*100000000 #Change G bytes into byte measurement. \n",
        "    else:\n",
        "      data_line[9] = float(data_line[9])*1000000 #Change M bytes into byte measurement. \n",
        "   \n",
        " \n",
        "  elif (data_line[13] == 'M'): #BPS measured in megabytes\n",
        "    #print(\"2\")\n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:13] + data_line[14:15]\n",
        "    data_line[9] = float(data_line[9])*1000000 #Change M bytes into byte measurement. \n",
        "  \n",
        "  elif data_line[11] == 'M': #Bytes measured in megabytes\n",
        "    #print(\"1\")\n",
        "    \n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:11] + data_line[12:15]\n",
        "    data_line[7] = float(data_line[7])*1000000 #Change M bytes into byte measurement. \n",
        "    \n",
        "  else: #No megabyte metrics\n",
        "    #print(\"0\")\n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:14]\n",
        "   \n",
        "\n",
        "  data_line  = np.asarray(data_line) #Turn each line into a NumPy array.\n",
        "  framedata.append(data_line) #append each line to 'mother' array. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IldG2fYf7dtL"
      },
      "source": [
        "5. Convert numpy array into pandas dataframe and add additional columns.\n",
        "  - Day: gives the day of the week as an integer. Monday is 0 and Sunday is 6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS4XVsJv4cbC"
      },
      "source": [
        "df = pd.DataFrame(np.array(framedata), columns=headings_line) #convert 2D numpy array framedata into pandas df. \n",
        "#df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "print(df) # Print data just to check it is formatted correctly."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ePYkBWYgO1U"
      },
      "source": [
        "#Examining column headings\n",
        "#print(df.columns)\n",
        "#Examining all unique IP addresses in sample.\n",
        "#print(pd.unique(df[\"SrcIPAddr:Port\"]))\n",
        "\n",
        "#Define all data types correctly. \n",
        "df = df.astype({\"Date\": np.datetime64})\n",
        "df[\"Day\"] = df['Date'].dt.dayofweek #Created Day variable. \n",
        "df = df.astype({'first-seen': np.datetime64})\n",
        "df = df.astype({'Duration': np.float64})\n",
        "df = df.astype({\"SrcIPAddr:Port\": str})\n",
        "df = df.astype({\"DstIPAddr:Port\": str})\n",
        "df = df.astype({\"Packets\": np.int64})\n",
        "df = df.astype({\"Bytes\": np.float64})\n",
        "df = df.astype({\"pps\": np.float64})\n",
        "df = df.astype({\"bps\": np.float64})\n",
        "df = df.astype({\"Bpp\": np.float64})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrT_zzX-vkNm"
      },
      "source": [
        "#display(df.dtypes)\n",
        "# Dropping the non numeric variables for now, need to find a way to encode them. Suggested: one-hot encoding. Could also be done in scaling step. \n",
        "df = (df.drop(['Date', 'first-seen', 'Proto', 'SrcIPAddr:Port', 'DstIPAddr:Port'], axis = 1))\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2FudlYFvCJC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "0468ee61-8b94-48ed-be4b-6b15a795351f"
      },
      "source": [
        "#A plot of Bytes vs time (treating bytes as target variable)\n",
        "plt.figure(figsize=(40,10))\n",
        "plt.title(\"Bytes vs Int Index\")\n",
        "plt.plot(r' df['Bytes']) df['Bytes'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5E3llr2wiQB"
      },
      "source": [
        "6. Split data into both training and test set. Use 80/20 split. \n",
        "   Decide later whether validation set or whether cross validation applied\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWcYnu049hVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d034df08-e31d-46a9-9042-70a1191cdd94"
      },
      "source": [
        "test_size = int(len(df) * 0.2) # the test data will be 20% (0.2) of the sample.\n",
        "train = df.iloc[:-test_size,:].copy()  #Not copying here threw an error. Must be careful not to keep two copies for memory reasons.\n",
        "test = df.iloc[-test_size:,:].copy() \n",
        "\n",
        "X_train = train.drop('Bytes',axis=1).copy() #Drop target variable from training data. \n",
        "y_train = train[['Bytes']].copy() # The double brackets are to keep Bytes in a pandas dataframe format, otherwise it will be pandas Series.\n",
        "print(X_train.shape, y_train.shape) #Check shape of training variables. \n",
        "\n",
        "# Output -> (400, 6) (400, 1)\n",
        "# Output implies we have 400 examples, with 6 predictors and 1 predicted value "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4HrUcbr3R6j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "746b7ba2-aab1-4c60-df13-d33bfadc9a1d"
      },
      "source": [
        "#Visualise split in sample\n",
        "plt.figure(figsize=(40,10))\n",
        "plt.title(\"Split of Test and Train Set using Bytes as target variable\")\n",
        "plt.plot(train.index,train['Bytes'],label='Training set');\n",
        "plt.plot(test.index,test['Bytes'],label='Test set')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BGIUeb_3JCl"
      },
      "source": [
        "Xscaler = MinMaxScaler(feature_range=(0, 1)) # scikit MinMixScaler allows all variables to be normalised between 0 and 1.\n",
        "Xscaler.fit(X_train) #Compute the minimum and maximum to be used for later scaling\n",
        "scaled_X_train = Xscaler.transform(X_train) #Scale features of X according to feature_range.\n",
        "\n",
        "print(X_train.shape) #X_train shape is the same as earlier but now scaled. \n",
        "print(scaled_X_train) #Demonstrate normalised data. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMobcpqrpmAY"
      },
      "source": [
        "Yscaler = MinMaxScaler(feature_range=(0, 1)) #apply same normalisation to response. \n",
        "Yscaler.fit(y_train)\n",
        "scaled_y_train = Yscaler.transform(y_train)\n",
        "print(scaled_y_train.shape) #Shape is constant. \n",
        "#scaled_y_train = scaled_y_train.reshape(-1) # remove the second dimention from y so the shape changes from (n,1) to (n,)\n",
        "print(scaled_y_train.shape)\n",
        "\n",
        "\n",
        "scaled_y_train = np.insert(scaled_y_train, 0, 0)\n",
        "print(scaled_y_train)\n",
        "scaled_y_train = np.delete(scaled_y_train, -1) #Why do we have to delete this?\n",
        "print(scaled_y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcdTgS6S3L8t"
      },
      "source": [
        "n_input = 100 #how many samples/rows/timesteps to look in the past in order to forecast the next sample\n",
        "n_features= X_train.shape[1] # how many predictors/Xs/features we have to predict y\n",
        "b_size = 32 # Number of timeseries samples in each batch\n",
        "train_generator = TimeseriesGenerator(scaled_X_train, scaled_y_train, length=n_input, batch_size=b_size)\n",
        "\n",
        "print(train_generator[0][0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqmDXFuAhFzJ"
      },
      "source": [
        "# Simple LSTM Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW_I4toi9-DO"
      },
      "source": [
        "7. Defining the Keras model configuaration for Simple LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukpmoRaA3OZN"
      },
      "source": [
        "simple_LSTM_model = Sequential() # The sequential argument means that we can add layers without worrying about the underlying shape of the tensors \n",
        "simple_LSTM_model.add(LSTM(50, activation='sigmoid', input_shape=(n_input, n_features))) # Model is an LSTM, 150 is dimentionality of the output, activation function is sigmoid\n",
        "simple_LSTM_model.add(Dense(1)) # Dense layer as the first layer of the model\n",
        "simple_LSTM_model.compile(optimizer='adam', loss='mse', metrics=['accuracy']) # Compile the model with the adam optimizer, loss measured in Mean Squarred Error and reporting on the accuracy of the model\n",
        "# Adam refers to the learning rate change, which is measured by the exponentially decaying average of past gradients\n",
        "simple_LSTM_model.summary() # Print out a summary of the LSTM to check that it was compiled correctly "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPJt-ITJHtcE"
      },
      "source": [
        "8. Fit the data to the model and train. Generate a summary of the model and show training results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3hK7I40Hr-A"
      },
      "source": [
        "simple_LSTM_model.fit(train_generator, epochs=100, verbose = 1) # Fit the features excluding target, and predict the target value\n",
        "# verbose of 0 hides the training, 2 shows the full log\n",
        "loss_per_epoch = simple_LSTM_model.history.history['loss']\n",
        "plt.plot(range(len(loss_per_epoch)), loss_per_epoch);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YU4rDGlK9DI"
      },
      "source": [
        "9. TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Sy1TcBMBLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9126c4-2691-40b9-8954-852c21024c38"
      },
      "source": [
        "X_test = test.drop('Bytes',axis=1).copy()\n",
        "#print(X_test)\n",
        "scaled_X_test = Xscaler.transform(X_test)\n",
        "#print(scaled_X_test)\n",
        "test_generator = TimeseriesGenerator(scaled_X_test, np.zeros(len(X_test)), length=25, batch_size=b_size) #There are only 17 samples in the test set so it cannot look back.\n",
        "print(test_generator[0][0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikSwTb0VjTAR"
      },
      "source": [
        "y_pred_scaled = simple_LSTM_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "simple_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[25:],'y_pred':y_pred.ravel()})\n",
        "simple_lstm_results.plot()\n",
        "print(simple_lstm_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9YMC-e-bVS2"
      },
      "source": [
        "Predictions made in Megabytes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dSsN3FlbX0e"
      },
      "source": [
        "y_pred_scaled = simple_LSTM_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "simple_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[25:]/1000000,'y_pred':y_pred.ravel()/1000000})\n",
        "simple_lstm_results.plot()\n",
        "print(simple_lstm_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu9GBgombnyp",
        "outputId": "07e4accb-938e-47e3-9384-2174ba0691ae"
      },
      "source": [
        "simple_lstm_results['residuals'] = np.square(simple_lstm_results.y_pred - simple_lstm_results.y_true)\n",
        "# results['residuals_squared'] = np.square(results.residuals)\n",
        "simple_LSTM_mse = simple_lstm_results.residuals.sum() * (1/len(simple_lstm_results))\n",
        "print(np.round(simple_LSTM_mse, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHd51XCpZTaT"
      },
      "source": [
        "# Bidirectional LSTM Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg65LqKNZqOw"
      },
      "source": [
        "10. Defining the Keras model configuaration for Bidirectional LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKV5YX2hZTG7"
      },
      "source": [
        "# Bidirectional LSTM supported in Keras using a layer wrapper \n",
        "# Common approach is to use concatenate, providing 2x outputs to next layer\n",
        "# Takes the first LSTM layer as an argument\n",
        "bidirectional_lstm_model = Sequential() # The sequential argument means that we can add layers without worrying about the underlying shape of the tensors \n",
        "bidirectional_lstm_model.add(Bidirectional(LSTM(50, return_sequences=False, activation=\"sigmoid\"), input_shape=(n_input, n_features)))\n",
        "bidirectional_lstm_model.add(Dense(1))\n",
        "bidirectional_lstm_model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOBwSV7ZCjSg"
      },
      "source": [
        "bidirectional_lstm_model.fit(train_generator, epochs=100, verbose=2)\n",
        "loss_per_epoch = bidirectional_lstm_model.history.history['loss']\n",
        "plt.plot(range(len(loss_per_epoch)), loss_per_epoch);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58GdL-YpErOC"
      },
      "source": [
        "y_pred_scaled = bidirectional_lstm_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "bidirectional_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[25:],'y_pred':y_pred.ravel()})\n",
        "print(bidirectional_lstm_results)\n",
        "bidirectional_lstm_results.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNLpyAyzYFkn"
      },
      "source": [
        "bidirectional_lstm_results['residuals'] = np.square(bidirectional_lstm_results.y_pred - bidirectional_lstm_results.y_true)\n",
        "# results['residuals_squared'] = np.square(results.residuals)\n",
        "bidirectional_LSTM_mse = bidirectional_lstm_results.residuals.sum() * (1/len(bidirectional_lstm_results))\n",
        "print(bidirectional_LSTM_mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoUVo7Yhb-W-"
      },
      "source": [
        "Predictions made in megabytes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_X_wcZocA62"
      },
      "source": [
        "y_pred_scaled = bidirectional_lstm_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "bidirectional_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[25:]/1000000,'y_pred':y_pred.ravel()/1000000})\n",
        "print(bidirectional_lstm_results)\n",
        "bidirectional_lstm_results.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNeM848kcLTT",
        "outputId": "50af2678-aae3-4dbd-ac79-1080db7707d4"
      },
      "source": [
        "bidirectional_lstm_results['residuals'] = np.square(bidirectional_lstm_results.y_pred - bidirectional_lstm_results.y_true)\n",
        "# results['residuals_squared'] = np.square(results.residuals)\n",
        "bidirectional_LSTM_mse = bidirectional_lstm_results.residuals.sum() * (1/len(bidirectional_lstm_results))\n",
        "print(np.round(bidirectional_LSTM_mse, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7L7w7oFCbfw"
      },
      "source": [
        "# Stacked LSTM Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIqJ3Qj6Cg49"
      },
      "source": [
        "stacked_lstm_model = Sequential() # The sequential argument means that we can add layers without worrying about the underlying shape of the tensors \n",
        "stacked_lstm_model.add(LSTM(50, return_sequences=True, activation=\"sigmoid\", input_shape=(n_input, n_features)))\n",
        "stacked_lstm_model.add(LSTM(50, return_sequences=True))\n",
        "stacked_lstm_model.add(LSTM(50, return_sequences=True))\n",
        "stacked_lstm_model.add(Dense(1))\n",
        "stacked_lstm_model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcNDyvGbDUPY"
      },
      "source": [
        "stacked_lstm_model.fit(train_generator, epochs=30, verbose=2)\n",
        "loss_per_epoch = stacked_lstm_model.history.history['loss']\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZCO8p0iExFt"
      },
      "source": [
        "y_pred_scaled = stacked_lstm_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "stacked_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[25:],'y_pred':y_pred.ravel()})\n",
        "print(stacked_lstm_results)\n",
        "stacked_lstm_results.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDkcbG4MYKPP"
      },
      "source": [
        "stacked_lstm_results['residuals'] = np.square(stacked_lstm_results.y_pred - stacked_lstm_results.y_true)\n",
        "# results['residuals_squared'] = np.square(results.residuals)\n",
        "stacked_LSTM_mse = stacked_lstm_results.residuals.sum() * (1/len(stacked_lstm_results))\n",
        "print(stacked_LSTM_mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT2KUuzFcX-J"
      },
      "source": [
        "Predictions made in megabytes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifFID__BcZFS"
      },
      "source": [
        "y_pred_scaled = stacked_lstm_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "stacked_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[25:]/1000000,'y_pred':y_pred.ravel()/1000000})\n",
        "print(stacked_lstm_results)\n",
        "stacked_lstm_results.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NBtUURRcaOc",
        "outputId": "d943111d-59bb-4868-d454-11b01720d7ef"
      },
      "source": [
        "stacked_lstm_results['residuals'] = np.square(stacked_lstm_results.y_pred - stacked_lstm_results.y_true)\n",
        "# results['residuals_squared'] = np.square(results.residuals)\n",
        "stacked_LSTM_mse = stacked_lstm_results.residuals.sum() * (1/len(stacked_lstm_results))\n",
        "print(np.round(stacked_LSTM_mse, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}