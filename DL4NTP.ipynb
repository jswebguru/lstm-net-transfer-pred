{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "DL4NTP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.2 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/fle1scha/DL4NTP/blob/main/DL4NTP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add all imports and print TensorFlow version to verify it is correct version"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "from datetime import datetime, date\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers  import LSTM\n",
        "from tensorflow.keras.layers  import TimeDistributed\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from numpy import array\n",
        "from numpy import cumsum\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from random import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "print(keras.__version__)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in Data"
      ],
      "metadata": {
        "id": "O545X4ogU63c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Mount Google Drive file system"
      ],
      "metadata": {
        "id": "apW7Ec7Q6cZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# from google.colab import drive # Hosting the file in Google Drive, need to mount the drive so it is accessible\n",
        "# # Currently Google forces an authorisation code, local runtime would rectify this\n",
        "# drive.mount('/content/gdrive', force_remount=True) #force_remount forces Google t"
      ],
      "outputs": [],
      "metadata": {
        "id": "6CHuCizHipju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4bc5d0-243f-48e3-c098-8ced3f965ee1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Read in SANREN dataset.\n",
        "with open('SANREN_large.txt') as f:\n",
        "  SANReN = f.readlines()"
      ],
      "outputs": [],
      "metadata": {
        "id": "bMMf07H7mA3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37963be7-c76c-4f9e-bc89-f12777bd2ba7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Data Preprocessing***"
      ],
      "metadata": {
        "id": "3QMDbVDehWb7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Clean dataframe headers and create labels.\n",
        "headings_line = SANReN[0].split()\n",
        "\n",
        "headings_line[4:7] = [''.join(headings_line[4:7])] #Merge 'Src', 'IP', and 'Addr:Port' \n",
        "headings_line[5:8] = [''.join(headings_line[5:8])] #Merge 'Dst', 'IP', and 'Addr:Port' \n",
        "headings_line = headings_line[0:6] + headings_line[8:13] #Remove 'Flags', 'Tos', and 'Flows'.\n",
        "\n",
        "print(headings_line)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Wn5UvaMAtfxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Clean time-series data points. \n",
        "framedata = []\n",
        "for i in range(1, 3001):\n",
        "  data_line = SANReN[i].split()\n",
        "\n",
        "  if (data_line[11] == \"M\" and (data_line[14] == 'M' or data_line[14] == 'G')): #Bytes and BPS in megabytes\n",
        "    #print(\"1 and 2\") \n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:11] + data_line[12:14] + data_line[15:16]\n",
        "    data_line[7] = float(data_line[7])*1000000 #Change M bytes into byte measurement. \n",
        "    if (data_line[9] == 'G'):\n",
        "      data_line[9] = float(data_line[9])*100000000 #Change G bytes into byte measurement. \n",
        "    else:\n",
        "      data_line[9] = float(data_line[9])*1000000 #Change M bytes into byte measurement. \n",
        "   \n",
        " \n",
        "  elif (data_line[13] == 'M'): #BPS measured in megabytes\n",
        "    #print(\"2\")\n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:13] + data_line[14:15]\n",
        "    data_line[9] = float(data_line[9])*1000000 #Change M bytes into byte measurement. \n",
        "  \n",
        "  elif data_line[11] == 'M': #Bytes measured in megabytes\n",
        "    #print(\"1\")\n",
        "    \n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:11] + data_line[12:15]\n",
        "    data_line[7] = float(data_line[7])*1000000 #Change M bytes into byte measurement. \n",
        "    \n",
        "  else: #No megabyte metrics\n",
        "    #print(\"0\")\n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:14]\n",
        "   \n",
        "\n",
        "  #data_line  = np.asarray(data_line) #Turn each line into a NumPy array.\n",
        "  framedata.append(data_line) #append each line to 'mother' array."
      ],
      "outputs": [],
      "metadata": {
        "id": "W6uYg9vVuEcc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Convert Numpy array into Pandas dataframe.\n",
        "df = pd.DataFrame(np.array(framedata), columns=headings_line) \n",
        "print(df.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "yS4XVsJv4cbC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Define all data types correctly. \n",
        "df['Datetimetemp'] = df['Date'] + 'T' + df['first-seen'] #Combine Date and first-seen\n",
        "df = df.astype({'Date': 'datetime64[ns]'})\n",
        "df[\"Day\"] = df['Date'].dt.dayofweek #Created Day variable.\n",
        "#df['Date'] = df['Date'].map(dt.datetime.toordinal)\n",
        "df = df.astype({'first-seen': np.datetime64})\n",
        "df = df.astype({'Duration': np.float64})\n",
        "df = df.astype({\"SrcIPAddr:Port\": str})\n",
        "df = df.astype({\"DstIPAddr:Port\": str})\n",
        "df = df.astype({\"Packets\": np.int64})\n",
        "df = df.astype({\"Bytes\": np.float64})\n",
        "df = df.astype({\"pps\": np.float64})\n",
        "df = df.astype({\"bps\": np.float64})\n",
        "df = df.astype({\"Bpp\": np.float64})\n",
        "\n",
        "#Create binary Weekend variable.\n",
        "df['Weekend'] = 0\n",
        "df.loc[df['Day'] == 5 , 'Weekend'] = 1\n",
        "df.loc[df['Day'] == 6 , 'Weekend'] = 1\n",
        "\n",
        "#Insert combined Datetime at front of dataframe.\n",
        "df.insert(0, 'Datetime', df['Datetimetemp'])\n",
        "df['Datetime'] = df.Datetime.astype('datetime64[ns]')\n",
        "df['Datetime'] = df.Datetime.astype('int64') #Convert Datetime into an integer representation. This is a deprecated method. \n",
        "\n",
        "#Define university holiday calender\n",
        "holidays = pd.date_range(start='2020-1-1', end='2020-3-14', freq = '1D')\n",
        "holidays = holidays.append(pd.date_range(start='2020-5-1', end='2020-5-9', freq='1D'))\n",
        "holidays = holidays.append(pd.date_range(start='2020-07-08', end='2020-08-02', freq='1D'))\n",
        "holidays = holidays.append(pd.date_range(start='2020-09-18', end='2020-09-27', freq='1D'))\n",
        "holidays = holidays.append(pd.date_range(start='2020-11-24', end='2020-12-31', freq='1D'))\n",
        "print(df['Date'])\n",
        "#Add Holiday column to dataframe.\n",
        "df['Holiday'] = 0\n",
        "df.loc[(df['Date']) == any(holidays.date), 'Holiday'] = 1 #Can't get this to work\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "3ePYkBWYgO1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Delete unused columns.\n",
        "del df['Date']\n",
        "del df['first-seen']\n",
        "del df['Datetimetemp']"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#One-Hot Encoding\n",
        "category_df = (df.copy().drop(['Day', 'Weekend', 'Holiday', 'SrcIPAddr:Port', 'DstIPAddr:Port', 'Datetime', 'Duration', 'Packets', 'Bytes', 'pps', 'bps', 'Bpp'], axis = 1))\n",
        "for x in category_df.columns:\n",
        "    #Printing unique values per categorical variable\n",
        "    print(x ,':', len(category_df[x].unique()))\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "#Transform data\n",
        "onehot = encoder.fit_transform(category_df)\n",
        "category_df = pd.DataFrame(np.array(onehot))\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#A plot of Bytes vs Datetime\n",
        "plt.figure(figsize=(40,10))\n",
        "plt.title(\"Bytes vs Datetime\")\n",
        "plt.scatter(df['Datetime'], df['Bytes']) #changed to scatter because line graph is very bunched. fromordinal only differentiates by date so will have to find a way to get it into seconds. \n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Add encoded categorical data to regression data\n",
        "df = (df.copy().drop(['Proto'], axis = 1)).copy()\n",
        "print(df.shape)\n",
        "df = pd.concat([df, category_df], axis = 1)\n",
        "print(df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "JrT_zzX-vkNm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Split data into both training and test set. Use 80/20 split.\n",
        "test_size = int(len(df) * 0.2) # the test data will be 20% (0.2) of the sample.\n",
        "train = df.iloc[:-test_size,:].copy()  #Not copying here threw an error. Must be careful not to keep two copies for memory reasons.\n",
        "test = df.iloc[-test_size:,:].copy() \n",
        "\n",
        "X_train = train.drop('Bytes',axis=1).copy() #Drop target variable from training data. \n",
        "y_train = train[['Bytes']].copy() # The double brackets are to keep Bytes in a pandas dataframe format, otherwise it will be pandas Series.\n",
        "print(X_train.shape, y_train.shape) #Check shape of training variables. "
      ],
      "outputs": [],
      "metadata": {
        "id": "JWcYnu049hVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d034df08-e31d-46a9-9042-70a1191cdd94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Visualise split in sample\n",
        "plt.figure(figsize=(40,10))\n",
        "plt.title(\"Split of Test and Train Set using Bytes as Target Variable\")\n",
        "plt.scatter(train['Datetime'],train['Bytes'],label='Training set')\n",
        "plt.scatter(test['Datetime'],test['Bytes'],label='Test set')\n",
        "plt.legend()"
      ],
      "outputs": [],
      "metadata": {
        "id": "q4HrUcbr3R6j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "746b7ba2-aab1-4c60-df13-d33bfadc9a1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Scale training dating\n",
        "Xscaler = MinMaxScaler(feature_range=(0, 1)) # scikit MinMixScaler allows all variables to be normalised between 0 and 1.\n",
        "Xscaler.fit(X_train) #Compute the minimum and maximum to be used for later scaling\n",
        "scaled_X_train = Xscaler.transform(X_train) #Scale features of X according to feature_range.\n",
        "\n",
        "print(X_train.shape) #X_train shape is the same as earlier but now scaled. \n",
        "print(scaled_X_train) #Demonstrate normalised data. "
      ],
      "outputs": [],
      "metadata": {
        "id": "8BGIUeb_3JCl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Scale training response\n",
        "Yscaler = MinMaxScaler(feature_range=(0, 1)) #apply same normalisation to response. \n",
        "Yscaler.fit(y_train)\n",
        "scaled_y_train = Yscaler.transform(y_train)\n",
        "scaled_y_train = np.insert(scaled_y_train, 0, 0)\n",
        "scaled_y_train = np.delete(scaled_y_train, -1)\n",
        "\n",
        "print(scaled_y_train.shape) #Shape is constant. "
      ],
      "outputs": [],
      "metadata": {
        "id": "XMobcpqrpmAY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Define input_shape for LSTM\n",
        "n_input = 100 #how many samples/rows/timesteps to look in the past in order to forecast the next sample\n",
        "n_features= X_train.shape[1] # how many predictors/Xs/features we have to predict y\n",
        "b_size = 100 # Number of timeseries samples in each batch\n",
        "train_generator = TimeseriesGenerator(scaled_X_train, scaled_y_train, length=n_input, batch_size=b_size)\n",
        "print(train_generator[0][0].shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "rcdTgS6S3L8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple LSTM Implementation"
      ],
      "metadata": {
        "id": "iqmDXFuAhFzJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# The sequential argument means that we can add layers without worrying about the underlying shape of the tensors\n",
        "simple_LSTM_model = Sequential() \n",
        "# Model is an LSTM, 50 is dimentionality of the output, activation function is relu\n",
        "simple_LSTM_model.add(LSTM(50, activation='relu', input_shape=(n_input, n_features))) \n",
        "# Dense layer as the first layer of the model\n",
        "simple_LSTM_model.add(Dense(1)) \n",
        "# Compile the model with the adam optimizer, loss measured in Mean Squarred Error\n",
        "# Adam refers to the learning rate change, which is measured by the exponentially decaying average of past gradients\n",
        "simple_LSTM_model.compile(optimizer='adam', loss='mse') \n",
        "# Print out a summary of the LSTM to check that it was compiled correctly \n",
        "simple_LSTM_model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "ukpmoRaA3OZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Fit the data to the model and train.\n",
        "simple_LSTM_model.fit(train_generator, epochs=20, verbose = 1) # Fit the features excluding target, and predict the target value\n",
        "# verbose of 0 hides the training, 2 shows the full log\n",
        "loss_per_epoch = simple_LSTM_model.history.history['loss']\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(len(loss_per_epoch)), loss_per_epoch)"
      ],
      "outputs": [],
      "metadata": {
        "id": "O3hK7I40Hr-A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Testing x data\n",
        "X_test = test.drop('Bytes', axis=1).copy()\n",
        "scaled_X_test = Xscaler.transform(X_test)\n",
        "test_generator = TimeseriesGenerator(scaled_X_test, np.zeros(len(X_test)), length=100, batch_size=b_size) #There are only 17 samples in the test set so it cannot look back.\n",
        "print(test_generator[0][0].shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "V9Sy1TcBMBLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9126c4-2691-40b9-8954-852c21024c38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Predicted LSTM response for bytes\n",
        "y_pred_scaled = simple_LSTM_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "simple_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[100:],'y_pred':y_pred.ravel()})\n",
        "simple_lstm_results.plot()\n",
        "print(simple_lstm_results)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ikSwTb0VjTAR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Predicted  LSTM response in megabytes\n",
        "simple_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[100:]/1000000,'y_pred':y_pred.ravel()/1000000})\n",
        "simple_lstm_results['residuals'] = np.square(\n",
        "    simple_lstm_results.y_pred - simple_lstm_results.y_true)\n",
        "# results['residuals_squared'] = np.square(results.residuals)\n",
        "simple_LSTM_mse = simple_lstm_results.residuals.sum() * (1/len(simple_lstm_results))\n",
        "print('MSE: ' + str(np.round(simple_LSTM_mse, 3)))"
      ],
      "outputs": [],
      "metadata": {
        "id": "6dSsN3FlbX0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional LSTM Implementation"
      ],
      "metadata": {
        "id": "zHd51XCpZTaT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Bidirectional LSTM supported in Keras using a layer wrapper \n",
        "# Common approach is to use concatenate, providing 2x outputs to next layer\n",
        "# Takes the first LSTM layer as an argument\n",
        "# The sequential argument means that we can add layers without worrying about the underlying shape of the tensors\n",
        "bidirectional_lstm_model = Sequential() \n",
        "bidirectional_lstm_model.add(Bidirectional(LSTM(50, return_sequences=False, activation=\"relu\"), input_shape=(n_input, n_features)))\n",
        "bidirectional_lstm_model.add(Dense(1))\n",
        "bidirectional_lstm_model.compile(loss='mse', optimizer='adam')\n",
        "bidirectional_lstm_model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "lKV5YX2hZTG7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bidirectional_lstm_model.fit(train_generator, epochs=20, verbose=2)\n",
        "loss_per_epoch = bidirectional_lstm_model.history.history['loss']\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(len(loss_per_epoch)), loss_per_epoch)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "KOBwSV7ZCjSg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_pred_scaled = bidirectional_lstm_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "bidirectional_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[100:],'y_pred':y_pred.ravel()})\n",
        "print(bidirectional_lstm_results)\n",
        "bidirectional_lstm_results.plot()"
      ],
      "outputs": [],
      "metadata": {
        "id": "58GdL-YpErOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions made in megabytes"
      ],
      "metadata": {
        "id": "hoUVo7Yhb-W-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bidirectional_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[100:]/1000000,'y_pred':y_pred.ravel()/1000000})\n",
        "bidirectional_lstm_results['residuals'] = np.square(bidirectional_lstm_results.y_pred - bidirectional_lstm_results.y_true)\n",
        "# results['residuals_squared'] = np.square(results.residuals)\n",
        "bidirectional_LSTM_mse = bidirectional_lstm_results.residuals.sum() * (1/len(bidirectional_lstm_results))\n",
        "print('MSE: ' + str(np.round(bidirectional_LSTM_mse, 3)))"
      ],
      "outputs": [],
      "metadata": {
        "id": "4_X_wcZocA62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacked LSTM Implementation"
      ],
      "metadata": {
        "id": "k7L7w7oFCbfw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "stacked_lstm_model = Sequential() # The sequential argument means that we can add layers without worrying about the underlying shape of the tensors \n",
        "stacked_lstm_model.add(LSTM(50, return_sequences=True, activation=\"relu\", input_shape=(n_input, n_features)))\n",
        "stacked_lstm_model.add(LSTM(50, return_sequences=True))\n",
        "stacked_lstm_model.add(LSTM(50))\n",
        "stacked_lstm_model.add(Dense(1))\n",
        "stacked_lstm_model.compile(loss='mse', optimizer='adam')\n",
        "stacked_lstm_model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "mIqJ3Qj6Cg49"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "stacked_lstm_model.fit(train_generator, epochs=20, verbose=2)\n",
        "loss_per_epoch = stacked_lstm_model.history.history['loss']\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(len(loss_per_epoch)), loss_per_epoch)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BcNDyvGbDUPY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_pred_scaled = stacked_lstm_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "stacked_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[100:],'y_pred':y_pred.ravel()})\n",
        "print(stacked_lstm_results)\n",
        "stacked_lstm_results.plot()"
      ],
      "outputs": [],
      "metadata": {
        "id": "cZCO8p0iExFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions made in megabytes"
      ],
      "metadata": {
        "id": "yT2KUuzFcX-J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "stacked_lstm_results = pd.DataFrame(\n",
        "    {'y_true': test['Bytes'].values[100:]/1000000, 'y_pred': y_pred.ravel()/1000000})\n",
        "stacked_lstm_results['residuals'] = np.square(\n",
        "    stacked_lstm_results.y_pred - stacked_lstm_results.y_true)\n",
        "stacked_LSTM_mse = stacked_lstm_results.residuals.sum() * \\\n",
        "    (1/len(stacked_lstm_results))\n",
        "print('MSE: ' + str(np.round(stacked_LSTM_mse, 3)))\n"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}