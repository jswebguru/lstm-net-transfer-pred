{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL4NTP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fle1scha/DL4NTP/blob/main/DL4NTP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cCDCp4LhGom"
      },
      "source": [
        "#Import tensorflow and numpy\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrwM3cXKhYmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086f3697-26ee-4080-b3c6-8449aa11779e"
      },
      "source": [
        "#Import keras models\n",
        "from random import random\n",
        "from numpy import array\n",
        "from numpy import cumsum\n",
        "\n",
        "from keras.layers import Bidirectional\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI4XzmhYM7mh"
      },
      "source": [
        "#Import scikit modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmltwXVFCzKO"
      },
      "source": [
        "#Import any other libraries needed\n",
        "import math\n",
        "import os\n",
        "from datetime import datetime, date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O545X4ogU63c"
      },
      "source": [
        "# Read in Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apW7Ec7Q6cZU"
      },
      "source": [
        "1. Mount Google Drive file system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CHuCizHipju"
      },
      "source": [
        "from google.colab import drive # Hosting the file in Google Drive, need to mount the drive so it is accessible\n",
        "# Currently Google forces an authorisation code, local runtime would rectify this\n",
        "drive.mount('/content/gdrive', force_remount=True) #force_remount forces Google t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWIZCG-u6Z12"
      },
      "source": [
        "2. Read in SANReN sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMMf07H7mA3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d34237-01f4-4959-cfa8-486998b3a0cd"
      },
      "source": [
        "with open('/content/gdrive/My Drive/SANReN.txt') as f:\n",
        "  SANReN = f.readlines()\n",
        "  \n",
        "#Iterate through first 5 rows to ensure data has been read correctly. \n",
        "for i in range(5):\n",
        "  print(SANReN[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Date first-seen          Duration Proto      Src IP Addr:Port          Dst IP Addr:Port   Flags Tos  Packets    Bytes      pps      bps    Bpp Flows\n",
            "\n",
            "2020-07-04 20:10:06.480     1.223 TCP       146.231.4.0:47837 ->   155.232.240.0:443   .A....   0     4500   234000     3679    1.5 M     52     1\n",
            "\n",
            "2020-07-04 20:09:01.555    78.205 TCP       196.24.45.0:443   ->   196.11.235.0:29108 .AP...   0    31000   46.5 M      396    4.8 M   1500     1\n",
            "\n",
            "2020-07-04 20:10:01.690     5.307 TCP        146.230.0.0:6474  ->     196.24.45.0:443   .A....   0     1000    40000      188    60297     40     1\n",
            "\n",
            "2020-07-04 20:09:23.019    43.982 TCP     197.102.66.0:443   ->   196.21.242.0:52855 .A....   0     2000    3.0 M       45   545677   1500     1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QMDbVDehWb7"
      },
      "source": [
        "# ***Data Preprocessing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U64MBm826WiF"
      },
      "source": [
        "3. Clean dataframe headers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn5UvaMAtfxu"
      },
      "source": [
        "headings_line = SANReN[0].split()\n",
        "#Merge 'Src', 'IP', and 'Addr:Port' \n",
        "headings_line[4:7] = [''.join(headings_line[4:7])]\n",
        "#Merge 'Dst', 'IP', and 'Addr:Port' \n",
        "headings_line[5:8] = [''.join(headings_line[5:8])]\n",
        "#Remove 'Flags', 'Tos', and 'Flows'.\n",
        "headings_line = headings_line[0:6] + headings_line[8:13]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwj9a3jl6K70"
      },
      "source": [
        "4. Clean time-series data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6uYg9vVuEcc"
      },
      "source": [
        "framedata = []\n",
        "for i in range(1, len(SANReN)):\n",
        "  data_line = SANReN[i].split()\n",
        "\n",
        "  if (data_line[11] == \"M\" and data_line[14] == 'M'): #Bytes and BPS in megabytes\n",
        "    #print(\"1 and 2\") \n",
        "    \n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:11] + data_line[12:14] + data_line[15:16]\n",
        "    data_line[7] = float(data_line[7])*1000000 #Change M bytes into byte measurement. \n",
        "    data_line[9] = float(data_line[9])*1000000 #Change M bytes into byte measurement. \n",
        "   \n",
        " \n",
        "  elif (data_line[13] == 'M'): #BPS measured in megabytes\n",
        "    #print(\"2\")\n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:13] + data_line[14:15]\n",
        "    data_line[9] = float(data_line[9])*1000000 #Change M bytes into byte measurement. \n",
        "  \n",
        "  elif data_line[11] == 'M': #Bytes measured in megabytes\n",
        "    #print(\"1\")\n",
        "    \n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:11] + data_line[12:15]\n",
        "    data_line[7] = float(data_line[7])*1000000 #Change M bytes into byte measurement. \n",
        "    \n",
        "  else: #No megabyte metrics\n",
        "    #print(\"0\")\n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:14]\n",
        "   \n",
        "\n",
        "  data_line  = np.asarray(data_line) \n",
        "  framedata.append(data_line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IldG2fYf7dtL"
      },
      "source": [
        "5. Convert numpy array into pandas dataframe and add additional columns.\n",
        "  - Day: gives the day of the week as an integer. Monday is 0 and Sunday is 6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS4XVsJv4cbC"
      },
      "source": [
        "df = pd.DataFrame(np.array(framedata), columns=headings_line)\n",
        "#df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "print(df) # Print data just to check it is formatted correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ePYkBWYgO1U"
      },
      "source": [
        "#Examining column headings\n",
        "#print(df.columns)\n",
        "#Examining all unique IP addresses in sample.\n",
        "#print(pd.unique(df[\"SrcIPAddr:Port\"]))\n",
        "\n",
        "#Define all data types correctly. \n",
        "df = df.astype({\"Date\": np.datetime64})\n",
        "df[\"Day\"] = df['Date'].dt.dayofweek\n",
        "df = df.astype({'first-seen': np.datetime64})\n",
        "df = df.astype({'Duration': np.float64})\n",
        "df = df.astype({\"SrcIPAddr:Port\": str})\n",
        "df = df.astype({\"DstIPAddr:Port\": str})\n",
        "df = df.astype({\"Packets\": np.int64})\n",
        "df = df.astype({\"Bytes\": np.float64})\n",
        "df = df.astype({\"pps\": np.float64})\n",
        "df = df.astype({\"bps\": np.float64})\n",
        "df = df.astype({\"Bpp\": np.float64})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrT_zzX-vkNm"
      },
      "source": [
        "#display(df.dtypes)\n",
        "# Dropping the non numeric variables for now, need to find a way to encode them\n",
        "df = (df.drop(['Date', 'first-seen', 'Proto', 'SrcIPAddr:Port', 'DstIPAddr:Port'], axis = 1))\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2FudlYFvCJC"
      },
      "source": [
        "#A plot of Bytes vs time \n",
        "plt.figure(figsize=(40,10))\n",
        "plt.title(\"Bytes vs Int Index\")\n",
        "plt.plot(range(len(df)), df['Bytes'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5E3llr2wiQB"
      },
      "source": [
        "6. Split data into both training and test set. Use 80/20 split. \n",
        "   Decide later whether validation set or whether cross validation applied\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWcYnu049hVO"
      },
      "source": [
        "test_size = int(len(df) * 0.2) # the test data will be 20% (0.2) of the entire data\n",
        "train = df.iloc[:-test_size,:].copy() \n",
        "# the copy() here is important, it will prevent us from getting: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_index,col_indexer] = value instead\n",
        "test = df.iloc[-test_size:,:].copy()\n",
        "\n",
        "X_train = train.drop('Bytes',axis=1).copy()\n",
        "y_train = train[['Bytes']].copy() # the double brakets here are to keep the y in a dataframe format, otherwise it will be pandas Series\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4HrUcbr3R6j"
      },
      "source": [
        "#Visualise split in sample\n",
        "plt.figure(figsize=(40,10))\n",
        "plt.title(\"Split of Test and Train Set using Bytes as target variable\")\n",
        "plt.plot(train.index,train['Bytes'],label='Training set');\n",
        "plt.plot(test.index,test['Bytes'],label='Test set')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BGIUeb_3JCl"
      },
      "source": [
        "Xscaler = MinMaxScaler(feature_range=(0, 1)) # scale so that all the X data will range from 0 to 1\n",
        "Xscaler.fit(X_train)\n",
        "scaled_X_train = Xscaler.transform(X_train)\n",
        "print(X_train.shape)\n",
        "Yscaler = MinMaxScaler(feature_range=(0, 1))\n",
        "Yscaler.fit(y_train)\n",
        "scaled_y_train = Yscaler.transform(y_train)\n",
        "print(scaled_y_train.shape)\n",
        "scaled_y_train = scaled_y_train.reshape(-1) # remove the second dimention from y so the shape changes from (n,1) to (n,)\n",
        "print(scaled_y_train.shape)\n",
        "\n",
        "scaled_y_train = np.insert(scaled_y_train, 0, 0)\n",
        "scaled_y_train = np.delete(scaled_y_train, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcdTgS6S3L8t"
      },
      "source": [
        "n_input = 25 #how many samples/rows/timesteps to look in the past in order to forecast the next sample\n",
        "n_features= X_train.shape[1] # how many predictors/Xs/features we have to predict y\n",
        "b_size = 32 # Number of timeseries samples in each batch\n",
        "train_generator = TimeseriesGenerator(scaled_X_train, scaled_y_train, length=n_input, batch_size=b_size)\n",
        "\n",
        "print(train_generator[0][0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqmDXFuAhFzJ"
      },
      "source": [
        "# Simple LSTM Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW_I4toi9-DO"
      },
      "source": [
        "7. Defining the Keras model configuaration for Simple LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukpmoRaA3OZN"
      },
      "source": [
        "simple_LSTM_model = Sequential() # The sequential argument means that we can add layers without worrying about the underlying shape of the tensors \n",
        "simple_LSTM_model.add(LSTM(150, activation='relu', input_shape=(n_input, n_features))) # Model is an LSTM, 150 is dimentionality of the output, activation function is sigmoid\n",
        "simple_LSTM_model.add(Dense(1)) # Dense layer as the first layer of the model\n",
        "simple_LSTM_model.compile(optimizer='adam', loss='mse', metrics=['accuracy']) # Compile the model with the adam optimizer, loss measured in Mean Squarred Error and reporting on the accuracy of the model\n",
        "# Adam refers to the learning rate change, which is measured by the exponentially decaying average of past gradients\n",
        "simple_LSTM_model.summary() # Print out a summary of the LSTM to check that it was compiled correctly "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPJt-ITJHtcE"
      },
      "source": [
        "8. Fit the data to the model and train. Generate a summary of the model and show training results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3hK7I40Hr-A"
      },
      "source": [
        "simple_LSTM_model.fit(train_generator, epochs=100, verbose = 2) # Fit the features excluding target, and predict the target value\n",
        "# verbose of 0 hides the training, 2 shows the full log\n",
        "loss_per_epoch = simple_LSTM_model.history.history['loss']\n",
        "plt.plot(range(len(loss_per_epoch)), loss_per_epoch);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YU4rDGlK9DI"
      },
      "source": [
        "9. TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Sy1TcBMBLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c9e2a9-f52f-4756-e1a0-7edc66641823"
      },
      "source": [
        "X_test = test.drop('Bytes',axis=1).copy()\n",
        "#print(X_test)\n",
        "scaled_X_test = Xscaler.transform(X_test)\n",
        "#print(scaled_X_test)\n",
        "test_generator = TimeseriesGenerator(scaled_X_test, np.zeros(len(X_test)), length=25, batch_size=b_size)\n",
        "print(test_generator[0][0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 25, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikSwTb0VjTAR"
      },
      "source": [
        "y_pred_scaled = simple_LSTM_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "results = pd.DataFrame({'y_true':test['Bytes'].values[25:],'y_pred':y_pred.ravel()})\n",
        "print(results)\n",
        "results.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rklPAUjxCj8x"
      },
      "source": [
        "9. Calculate prediction metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caBVol5hCl9C"
      },
      "source": [
        "simple_LSTM_training_mse = np.square(np.subtract(training_set[0], simple_LSTM_training_predictions[:,0])) #We want to calculate the difference between our predicted and actual values\n",
        "simple_LSTM_test_mse = np.square(np.subtract(test_set[0], simple_LSTM_test_predictions[:,0]))\n",
        "print('Train Score: %.2f MSE' % (simple_LSTM_training_mse))\n",
        "simple_LSTM_testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Test Score: %.2f MSE' % (simple_LSTM_test_mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHd51XCpZTaT"
      },
      "source": [
        "# Bidirectional LSTM Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg65LqKNZqOw"
      },
      "source": [
        "10. Defining the Keras model configuaration for Bidirectional LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKV5YX2hZTG7"
      },
      "source": [
        "# Bidirectional LSTM supported in Keras using a layer wrapper \n",
        "# Common approach is to use concatenate, providing 2x outputs to next layer\n",
        "# Takes the first LSTM layer as an argument\n",
        "bidirectional_lstm_model = Sequential() # The sequential argument means that we can add layers without worrying about the underlying shape of the tensors \n",
        "bidirectional_lstm_model.add(Bidirectional(LSTM(50, return_sequences=False, activation=\"sigmoid\"), input_shape=(n_input, n_features)))\n",
        "bidirectional_lstm_model.add(Dense(1))\n",
        "bidirectional_lstm_model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOBwSV7ZCjSg"
      },
      "source": [
        "bidirectional_lstm_model.fit(train_generator, epochs=100, verbose=2)\n",
        "loss_per_epoch = bidirectional_lstm_model.history.history['loss']\n",
        "plt.plot(range(len(loss_per_epoch)), loss_per_epoch);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58GdL-YpErOC"
      },
      "source": [
        "y_pred_scaled = bidirectional_lstm_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "results = pd.DataFrame({'y_true':test['Bytes'].values[25:],'y_pred':y_pred.ravel()})\n",
        "print(results)\n",
        "results.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7L7w7oFCbfw"
      },
      "source": [
        "# Stacked LSTM Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIqJ3Qj6Cg49"
      },
      "source": [
        "stacked_lstm_model = Sequential() # The sequential argument means that we can add layers without worrying about the underlying shape of the tensors \n",
        "stacked_lstm_model.add(LSTM(50, return_sequences=True, activation=\"sigmoid\", input_shape=(n_input, n_features)))\n",
        "stacked_lstm_model.add(LSTM(50, return_sequences=True))\n",
        "stacked_lstm_model.add(LSTM(50, return_sequences=True))\n",
        "stacked_lstm_model.add(LSTM(50))\n",
        "stacked_lstm_model.add(Dense(1))\n",
        "stacked_lstm_model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcNDyvGbDUPY"
      },
      "source": [
        "stacked_lstm_model.fit(train_generator, epochs=100, verbose=2)\n",
        "loss_per_epoch = stacked_lstm_model.history.history['loss']\n",
        "plt.plot(range(len(loss_per_epoch)), loss_per_epoch);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZCO8p0iExFt"
      },
      "source": [
        "y_pred_scaled = stacked_lstm_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "results = pd.DataFrame({'y_true':test['Bytes'].values[25:],'y_pred':y_pred.ravel()})\n",
        "print(results)\n",
        "results.plot()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}